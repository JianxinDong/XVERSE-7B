<div align="center">
<h1>
  XVERSE-7B
</h1>
</div>

<p align="center">
        <a href="https://huggingface.co/xverse/XVERSE-7B">ü§ó XVERSE-7B</a>&nbspÔΩú&nbsp<a href="https://huggingface.co/xverse/XVERSE-7B-Chat">ü§ó XVERSE-7B-Chat</a>&nbspÔΩú&nbsp
        <a href="https://modelscope.cn/organization/xverse" rel="nofollow"><img src="resources/modelscope.png" width="20px" style="max-width: 100%;"> ModelScope</a>&nbspÔΩú&nbsp
        <a href="resources/wechat.png">üí¨ ÂæÆ‰ø°Á§æÂå∫</a>
</p>

<h4 align="left">
    <p>
        <a href="README.md">‰∏≠Êñá</a> |
        <b>English</b>
    <p>
</h4>

## Model Introduction

**XVERSE-7B** is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. The models released this time include the base model **XVERSE-7B** and the chat model **XVERSE-7B-Chat**. Its key features are as follows:

- **Model Structure**: XVERSE-7B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
- **Training Data**: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
- **Tokenization**: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
- **Training Framework**: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.

## Model Evaluation

In order to validate the various abilities of the model, we have chosen several comprehensive capability benchmarks across multiple disciplines, including [MMLU](https://arxiv.org/abs/2009.03300) (English), [C-Eval](https://cevalbenchmark.com/) (Chinese), [AGIEval](https://arxiv.org/abs/2304.06364) (Chinese and English), [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) (Chinese and English), [GAOKAO-English](https://github.com/ExpressAI/AI-Gaokao) (English), the evaluation results are as follows (the bolded score represent the best performances):
|       Models       |    Type    |       MMLU       |      C-Eval      | AGIEval<sup>1</sup> | GAOKAO-Bench<sup>1</sup> | GAOKAO-English<sup>1</sup> |
| :----------------: | :--------: | :--------------: | :--------------: | :-----------------: | :----------------------: | :------------------------: |
|    Baichuan-7B     | pretrained | 42.3<sup>2</sup> | 42.8<sup>2</sup> |  34.4<sup>2</sup>   |     36.3<sup>2</sup>     |            44.3            |
| Baichuan2-7B-Base  | pretrained | 54.2<sup>2</sup> | 54.0<sup>2</sup> |  42.7<sup>2</sup>   |     47.5<sup>2</sup>     |            53.1            |
| Baichuan2-7B-Chat  | fine-tuned |       53.2       |       52.2       |        41.3         |           49.7           |            66.6            |
|    ChatGLM2-6B     | fine-tuned | 45.5<sup>2</sup> | 50.1<sup>2</sup> |        42.6         |           54.2           |            59.7            |
|     Falcon-7B      | pretrained | 27.8<sup>2</sup> |       25.8       |        26.2         |           26.3           |            29.9            |
|    InternLM-7B     | pretrained | 51.0<sup>2</sup> |       52.4       |        34.1         |           53.6           |            32.3            |
|  InternLM-7B-Chat  | fine-tuned | 50.8<sup>2</sup> |       52.8       |        39.0         |         **67.4**         |            43.9            |
|      Llama-7B      | pretrained | 35.1<sup>2</sup> |       27.0       |        27.4         |           26.0           |            30.1            |
|     Llama-2-7B     | pretrained | 45.3<sup>2</sup> |       28.9       |        27.0         |           27.8           |            47.8            |
|       MPT-7B       | pretrained | 29.6<sup>2</sup> |       27.8       |        24.2         |           25.3           |            28.1            |
|   Vicuna-7B-v1.5   | fine-tuned | 49.8<sup>2</sup> |       22.9       |        26.7         |           24.4           |            61.1            |
|   **XVERSE-7B**    | pretrained |       56.6       |     **57.1**     |        46.9         |           61.7           |            71.1            |
| **XVERSE-7B-Chat** | fine-tuned |     **63.7**     |       55.4       |      **48.9**       |           57.5           |          **78.2**          |

> <sup>1: Tests are conducted only on single-answer multiple-choice questions, thus excluding fill-in-the-blanks, open-ended questions, and multiple-answer multiple-choice questions.</sup>   
> <sup>2: Reporting results from official results of each model.</sup>   
>
> For MMLU, we adopt the [evaluation tools](https://github.com/hendrycks/test) provided by the authors, C-Eval, AGIEval, GAOKAO-Bench, GAOKAO-English are the same as MMLU, and uniformly use **5-shot** to construct the test samples.

### MMLU Category Results
|       Models       |    Type    | Average  |   STEM   | Social Science | Humanities |  Others  |
| :----------------: | :--------: | :------: | :------: | :------------: | :--------: | :------: |
|    Baichuan-7B     | pretrained |   42.3   |   35.6   |      48.9      |    38.4    |   48.1   |
| Baichuan2-7B-Chat  | fine-tuned |   53.2   |   43.1   |      59.1      |    50.0    |   59.1   |
|    ChatGLM2-6B     | fine-tuned |   45.5   |   40.1   |      51.6      |    41.2    |   51.2   |
|    InternLM-7B     | pretrained |   51.0   | **58.7** |      43.5      |    52.7    |   53.2   |
|      LLaMA-7B      | pretrained |   35.1   |   30.5   |      38.3      |    34.0    |   38.1   |
|     LLaMA2-7B      | pretrained |   45.3   |   36.4   |      51.2      |    42.9    |   52.2   |
|   **XVERSE-7B**    | pretrained |   56.6   |   45.6   |      65.3      |    50.4    |   65.5   |
| **XVERSE-7B-Chat** | fine-tuned | **63.7** |   51.7   |    **72.5**    |  **58.2**  | **72.2** |

### C-Eval Category Results
|       Models       |    Type    | Average  |   STEM   | Social Science | Humanities |  Others  |
| :----------------: | :--------: | :------: | :------: | :------------: | :--------: | :------: |
|    Baichuan-7B     | pretrained |   42.8   |   38.2   |      52.0      |    46.2    |   39.3   |
| Baichuan2-7B-Base  | pretrained |   54.9   |   47.9   |      67.3      |    58.4    |   52.8   |
| Baichuan2-7B-Chat  | fine-tuned |   52.2   |   44.6   |      65.0      |    55.8    |   50.9   |
|    ChatGLM2-6B     | fine-tuned |   50.1   |   46.4   |      60.4      |    50.6    |   46.9   |
|     Falcon-7B      | pretrained |   25.8   |   25.8   |      26.0      |    25.8    |   25.7   |
|    InternLM-7B     | pretrained |   52.4   |   47.0   |      64.9      |    55.6    |   47.6   |
|  InternLM-7B-Chat  | fine-tuned |   52.8   |   48.4   |      65.6      |    57.0    |   45.0   |
|      LLaMA-7B      | pretrained |   27.0   |   26.7   |      26.7      |    28.4    |   26.2   |
|     LLaMA2-7B      | pretrained |   28.9   |   26.8   |      34.5      |    30.0    |   26.4   |
|       MPT-7B       | pretrained |   27.8   |   27.4   |      29.8      |    26.9    |   27.7   |
|   Vicuna-7B-v1.5   | fine-tuned |   22.9   |   21.8   |      23.3      |    24.0    |   23.3   |
|   **XVERSE-7B**    | pretrained | **57.1** | **48.9** |    **71.0**    |  **59.7**  | **56.7** |
| **XVERSE-7B-Chat** | fine-tuned |   55.4   |   47.9   |      68.5      |    57.3    |   55.1   |

## Usage

### Environment Setup

1. Clone this repository:

```shell
git clone https://github.com/xverse-ai/XVERSE-7B
cd XVERSE-7B
```

2. Install the dependencies using pip:

```shell
pip install -r requirements.txt
```

### Loading with Transformers

The XVERSE-7B model can be loaded to chat for inference using the following code:

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> from transformers.generation.utils import GenerationConfig
>>> model_path = "xverse/XVERSE-7B-Chat"
>>> tokenizer = AutoTokenizer.from_pretrained(model_path)
>>> model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
>>> model.generation_config = GenerationConfig.from_pretrained(model_path)
>>> model = model.eval()
>>> history = [{"role": "user", "content": "1955Âπ¥Ë∞ÅÊòØÁæéÂõΩÊÄªÁªüÔºü‰ªñÊòØ‰ªÄ‰πàÂÖöÊ¥æÔºü"}]
>>> response = model.chat(tokenizer, history)
>>> print(response)
1955Âπ¥ÁöÑÁæéÂõΩÊÄªÁªüÊòØÂæ∑ÊÄÄÁâπ¬∑D¬∑ËâæÊ£ÆË±™Â®ÅÂ∞îÔºå‰ªñÊòØÂÖ±ÂíåÂÖöÁöÑÊàêÂëò„ÄÇ
>>> history.append({"role": "assistant", "content": response})
>>> history.append({"role": "user", "content": "‰ªñ‰ªªËÅå‰∫ÜÂ§öÂ∞ëÂπ¥"})
>>> response = model.chat(tokenizer, history)
>>> print(response)
Âæ∑ÊÄÄÁâπ¬∑D¬∑ËâæÊ£ÆË±™Â®ÅÂ∞îÂú®1953Âπ¥Âà∞1961Âπ¥ÊãÖ‰ªªÁæéÂõΩÊÄªÁªüÔºåÊâÄ‰ª•‰ªñ‰ªªËÅå‰∫Ü8Âπ¥„ÄÇ
```

### Web Demo

The following code can be used to start a web server. By entering the access address in the browser, you can perform inference with the XVERSE-7B model:

```shell
python chat_demo.py --port='port' --model_path='/path/to/model/' --tokenizer_path='/path/to/tokenizer/'
```

### XVERSE-7B-Chat Output Example
Below are some examples of XVERSE-7B-Chat obtained using `chat_demo.py`:

<details><summary><b>Role Play</b></summary>

```
Áî®Êà∑:
ÊâÆÊºîÊù®Ëøá

XVERSE-7B-Chat:
Â•ΩÁöÑÔºåÊàëÂ∞ÜÊâÆÊºîÊù®Ëøá„ÄÇÁé∞Âú®‰Ω†ÂèØ‰ª•ÂíåÊàëÂºÄÂßãÂØπËØù‰∫Ü„ÄÇ

Áî®Êà∑:
‰Ω†ÊòØË∞Å

XVERSE-7B-Chat:
ÊàëÊòØÊù®ËøáÔºåÂ∞èÈæôÂ•≥ÁöÑÂæíÂºüÔºå‰πüÊòØÁ•ûÈõïÂ§ß‰æ†„ÄÇ

Áî®Êà∑:
‰Ω†ÁöÑÂ¶ªÂ≠êÊòØË∞Å

XVERSE-7B-Chat:
ÊàëÁöÑÂ¶ªÂ≠êÊòØÂ∞èÈæôÂ•≥„ÄÇ

Áî®Êà∑:
ÈÄÄÂá∫ËßíËâ≤ÊâÆÊºî

XVERSE-7B-Chat:
ÂΩìÁÑ∂ÔºåÂ¶ÇÊûú‰Ω†ËøòÊúâÂÖ∂‰ªñÈóÆÈ¢òÔºåÈöèÊó∂ÈóÆÊàëÂ∞±Ë°å„ÄÇ

Áî®Êà∑:
‰Ω†ÊòØË∞Å

XVERSE-7B-Chat:
ÊàëÊòØ‰∏Ä‰∏™‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºåÂèØ‰ª•Â∏ÆÂä©ÂõûÁ≠îÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ø°ÊÅØÁ≠âÁ≠â„ÄÇ
```
</details>

<details><summary><b>Quiz</b></summary>

```
Áî®Êà∑:
„Ää‰∫∫Á±ªÁÆÄÂè≤„ÄãËøôÊú¨‰π¶ÁöÑ‰ΩúËÄÖÊòØË∞ÅÔºüËØ∑ÁÆÄË¶ÅÊ¶ÇÊã¨‰∏Ä‰∏ãËøôÊú¨‰π¶ÁöÑÂÜÖÂÆπ

XVERSE-7B-Chat:
„Ää‰∫∫Á±ªÁÆÄÂè≤„ÄãËøôÊú¨‰π¶ÁöÑ‰ΩúËÄÖÊòØ‰ª•Ëâ≤ÂàóÂéÜÂè≤Â≠¶ÂÆ∂Â∞§Áì¶Â∞î¬∑Ëµ´ÊãâÂà©„ÄÇËøôÊú¨‰π¶ÊòØ‰∏ÄÈÉ®ÂÖ≥‰∫é‰∫∫Á±ªÂéÜÂè≤ÁöÑÈÄö‰øóËØªÁâ©ÔºåÂÆÉ‰ªéÂ§ßÁàÜÁÇ∏ÂºÄÂßãËÆ≤Ëµ∑Ôºå‰∏ÄÁõ¥Âà∞Áé∞‰ª£ÁßëÊäÄÂØπ‰∫∫Á±ªÁ§æ‰ºöÁöÑÂΩ±ÂìçÔºåÂÖ®Êñπ‰ΩçÂú∞Êè≠Á§∫‰∫Ü‰∫∫Á±ªÂéÜÂè≤ÁöÑÂèëÂ±ïËÑâÁªúÂíåÊºîÂèòËøáÁ®ã„ÄÇ

Âú®„Ää‰∫∫Á±ªÁÆÄÂè≤„Äã‰∏≠ÔºåÂ∞§Áì¶Â∞î¬∑Ëµ´ÊãâÂà©ÊèêÂá∫‰∫ÜËÆ∏Â§öÁã¨ÁâπËÄåÊ∑±ÂàªÁöÑËßÇÁÇπÔºåÂ¶Ç‚ÄúÂéÜÂè≤ÁöÑÁªàÁªì‚ÄùÁêÜËÆ∫„ÄÅ‚Äú‰∫∫Á±ª‚ÄùÁöÑÂÆö‰πâ‰ª•Âèä‚Äú‰∫∫Á±ª‚Äù‰∏éÂÖ∂‰ªñÁîüÁâ©ÁöÑÂå∫Âà´Á≠â„ÄÇ‰ªñËÆ§‰∏∫Ôºå‰∫∫Á±ªÂπ∂ÈùûÁîüÊù•Â∞±ÊúâËá™ÊàëÊÑèËØÜÔºåËÄåÊòØÁî±‰∫éËØ≠Ë®ÄÁöÑÂá∫Áé∞Ôºå‰ΩøÂæóÊàë‰ª¨ËÉΩÂ§üÂ∞ÜËá™Â∑±ÁöÑÊÉ≥Ê≥ï‰º†ÈÄíÁªô‰ªñ‰∫∫Ôºå‰ªéËÄåÂΩ¢ÊàêÂÖ±Âêå‰ΩìÔºåËøõËÄå‰∫ßÁîü‰∫ÜÊñáÊòé„ÄÇÂêåÊó∂Ôºå‰ªñËøòÊåáÂá∫ÔºåÁßëÊäÄÁöÑÂèëÂ±ïËôΩÁÑ∂Â∏¶Êù•‰∫ÜËØ∏Â§öÂ•ΩÂ§ÑÔºå‰ΩÜ‰πüÂèØËÉΩÂØºËá¥‰∫∫Á±ªÂ§±ÂéªÂØπËá™ÁÑ∂ÁïåÁöÑÊéßÂà∂ÔºåÁîöËá≥ÂèØËÉΩÂºïÂèë‰∏ÄÂú∫ÂÖ®ÁêÉÊÄßÁöÑÂ§ßÁÅæÈöæ„ÄÇ
```
</details>

<details><summary><b>Text Generation</b></summary>

```
Áî®Êà∑:
ÊàëË¶ÅÂéªÊ∑±Âú≥ÊóÖÊ∏∏ÔºåÂ∏ÆÊàëÂà∂ÂÆö‰∏Ä‰ªΩ5Êó•Ê∏∏ËÆ°Âàí

XVERSE-7B-Chat:
‰ª•‰∏ãÊòØ‰∏Ä‰ªΩ‰∏∫Êúü‰∫îÂ§©ÁöÑÊ∑±Âú≥ÊóÖÊ∏∏Ë°åÁ®ãÂÆâÊéíÔºö

Á¨¨‰∏ÄÂ§©ÔºöÊ∑±Âú≥Â∏ÇÂå∫Ê∏∏
‰∏äÂçàÔºöÂèÇËßÇÊ∑±Âú≥ÂçöÁâ©È¶ÜÔºàÂÖçË¥πÔºâÔºå‰∫ÜËß£Ê∑±Âú≥ÁöÑÂéÜÂè≤ÂíåÊñáÂåñ„ÄÇ
‰∏≠ÂçàÔºöÂú®ÂçéÂº∫ÂåóÁæéÈ£üË°óÂìÅÂ∞ùÂêÑÁßçÂú∞ÈÅìÁöÑÂπø‰∏úÁæéÈ£ü„ÄÇ
‰∏ãÂçàÔºöÂâçÂæÄ‰∏ñÁïå‰πãÁ™óÔºàÈó®Á•®Á∫¶180ÂÖÉÔºâÔºåÊ¨£ËµèÂÖ®ÁêÉÂêÑÂú∞ÂêçËÉúÂè§ËøπÁöÑÁº©Â∞èÁâàÊ®°Âûã„ÄÇ
Êôö‰∏äÔºöÈÄõÊ∑±Âú≥ÊπæÂÖ¨Âõ≠ÔºåÊ¨£ËµèÊ∑±Âú≥ÊπæË∑®Êµ∑Â§ßÊ°•Â§úÊôØ„ÄÇ

Á¨¨‰∫åÂ§©Ôºö‰∏úÈÉ®Âçé‰æ®ÂüéÊ∏∏
‰∏äÂçàÔºöÂèÇËßÇÂ§ßÊ¢ÖÊ≤ôÊµ∑Êª®ÂÖ¨Âõ≠Ôºå‰∫´ÂèóÈò≥ÂÖâ„ÄÅÊ≤ôÊª©ÂíåÊµ∑Êµ™„ÄÇ
‰∏≠ÂçàÔºöÂú®ÈôÑËøëÁöÑÈ§êÂéÖÁî®ÂçàÈ§ê„ÄÇ
‰∏ãÂçàÔºöÂâçÂæÄ‰∏úÈÉ®Âçé‰æ®ÂüéÔºàÈó®Á•®Á∫¶90ÂÖÉÔºâÔºå‰ΩìÈ™åËå∂Ê∫™Ë∞∑ÁöÑËå∂ÊñáÂåñÔºåÂèÇËßÇÂ§ß‰æ†Ë∞∑ÁöÑËá™ÁÑ∂È£éÂÖâ„ÄÇ
Êôö‰∏äÔºöËøîÂõûÂ∏ÇÂå∫ÔºåÂú®Ê¨¢‰πêÊµ∑Â≤∏‰∫´ÂèóÂ§úÁîüÊ¥ª„ÄÇ

Á¨¨‰∏âÂ§©ÔºöÊ∑±Âú≥ÈáéÁîüÂä®Áâ©Âõ≠Ê∏∏
‰∏äÂçàÔºöÂâçÂæÄÊ∑±Âú≥ÈáéÁîüÂä®Áâ©Âõ≠ÔºàÈó®Á•®Á∫¶70ÂÖÉÔºâÔºå‰∏éÂêÑÁßçÁèçÁ®ÄÂä®Áâ©‰∫≤ÂØÜÊé•Ëß¶„ÄÇ
‰∏≠ÂçàÔºöÂú®Âä®Áâ©Âõ≠ÂÜÖÁöÑÈ§êÂéÖÁî®ÂçàÈ§ê„ÄÇ
‰∏ãÂçàÔºöÂèÇËßÇÊµ∑Ê¥ãÁéãÂõΩÔºàÈó®Á•®Á∫¶230ÂÖÉÔºâÔºåËßÇÁúãÁ≤æÂΩ©ÁöÑÊµ∑Ë±öË°®ÊºîÂíåÊµ∑ÁãÆË°®Êºî„ÄÇ
Êôö‰∏äÔºöËøîÂõûÂ∏ÇÂå∫ÔºåËá™Áî±Ê¥ªÂä®„ÄÇ

Á¨¨ÂõõÂ§©ÔºöÊ∑±Âú≥Â§ßÂ≠¶ÂèäÂë®ËæπÊ∏∏
‰∏äÂçàÔºöÂèÇËßÇÊ∑±Âú≥Â§ßÂ≠¶ÔºàÂÖçË¥πÔºâÔºåÊÑüÂèóÊ†°Âõ≠ÁöÑÂ≠¶ÊúØÊ∞õÂõ¥„ÄÇ
‰∏≠ÂçàÔºöÂú®ÈôÑËøëÁöÑÈ§êÂéÖÁî®ÂçàÈ§ê„ÄÇ
‰∏ãÂçàÔºöÂâçÂæÄÂçóÂ±±ÊñáÂåñÁîüÊÄÅÂõ≠ÔºàÈó®Á•®Á∫¶40ÂÖÉÔºâÔºåÊ¨£ËµèÂçóÂ±±ÁöÑËá™ÁÑ∂È£éÂÖâÂíå‰∫∫ÊñáÊôØËßÇ„ÄÇ
Êôö‰∏äÔºöËøîÂõûÂ∏ÇÂå∫ÔºåËá™Áî±Ê¥ªÂä®„ÄÇ

Á¨¨‰∫îÂ§©ÔºöË¥≠Áâ©‰ºëÈó≤Ê∏∏
‰∏äÂçàÔºöÂâçÂæÄÊ∑±Âú≥ÂõΩÈôÖË¥≠Áâ©‰∏≠ÂøÉÊàñÂçéÊ∂¶‰∏áË±°ÂüéÁ≠âÂ§ßÂûãË¥≠Áâ©‰∏≠ÂøÉË¥≠Áâ©„ÄÇ
‰∏≠ÂçàÔºöÂú®Ë¥≠Áâ©‰∏≠ÂøÉÂÜÖÁöÑÈ§êÂéÖÁî®ÂçàÈ§ê„ÄÇ
‰∏ãÂçàÔºöÂú®Ë¥≠Áâ©‰∏≠ÂøÉÁªßÁª≠Ë¥≠Áâ©ÊàñËÄÖ‰ºëÊÅØ„ÄÇ
Êôö‰∏äÔºöËøîÂõûÈÖíÂ∫ó‰ºëÊÅØÔºåÂáÜÂ§áÁ¶ªÂºÄÊ∑±Âú≥„ÄÇ
‰ª•‰∏äÂ∞±ÊòØ‰∏∫ÊÇ®Âà∂ÂÆöÁöÑÊ∑±Âú≥‰∫îÊó•Ê∏∏ËÆ°ÂàíÔºåÂ∏åÊúõÂØπÊÇ®ÁöÑÊóÖË°åÊúâÊâÄÂ∏ÆÂä©„ÄÇ
```
</details>

<details><summary><b>Programming Ability</b></summary>

```
Áî®Êà∑:
ÂÜô‰∏Ä‰∏™Âø´ÈÄüÊéíÂ∫è‰ª£Á†Å

XVERSE-7B-Chat:
ËøôÊòØ‰∏Ä‰∏™‰ΩøÁî®PythonÁºñÂÜôÁöÑÂø´ÈÄüÊéíÂ∫èÁöÑÁÆÄÂçïÁ§∫‰æãÔºö

def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)

print(quicksort([3,6,8,10,1,2,1]))
# Output: [1, 1, 2, 3, 6, 8, 10]
Ëøô‰∏™ÂáΩÊï∞È¶ñÂÖàÊ£ÄÊü•ÂàóË°®ÁöÑÈïøÂ∫¶ÊòØÂê¶Â∞è‰∫éÊàñÁ≠â‰∫é1„ÄÇÂ¶ÇÊûúÊòØÔºåÈÇ£‰πàÂÆÉÂ∞±ËøîÂõûËØ•ÂàóË°®„ÄÇÁÑ∂ÂêéÔºåÂÆÉÈÄâÊã©ÂàóË°®‰∏≠Èó¥ÁöÑÂÖÉÁ¥†‰Ωú‰∏∫Êû¢ËΩ¥„ÄÇÊé•ÁùÄÔºåÂÆÉÂàõÂª∫‰∏â‰∏™Á©∫ÂàóË°®Ôºöleft„ÄÅmiddleÂíåright„ÄÇleftÂåÖÂê´ÊâÄÊúâÂ∞è‰∫éÊû¢ËΩ¥ÁöÑÂÖÉÁ¥†ÔºåmiddleÂåÖÂê´ÊâÄÊúâÁ≠â‰∫éÊû¢ËΩ¥ÁöÑÂÖÉÁ¥†ÔºåËÄårightÂåÖÂê´ÊâÄÊúâÂ§ß‰∫éÊû¢ËΩ¥ÁöÑÂÖÉÁ¥†„ÄÇÊúÄÂêéÔºåÂÆÉÈÄíÂΩíÂú∞ÂØπÂ∑¶Âè≥‰∏§ÈÉ®ÂàÜËøõË°åÂêåÊ†∑ÁöÑÊìç‰ΩúÔºåÂπ∂Â∞ÜÁªìÊûúËøûÊé•Ëµ∑Êù•„ÄÇ
```
</details>

<details><summary><b>Math Ability</b></summary>

```
Áî®Êà∑:
Ë©πÂßÜÊñØÂÜ≥ÂÆöÊØèÂë®Ë∑ë 3 Ê¨° 3 ÊÆµÂÜ≤Âà∫Ë∑ëÔºåÊØèÊÆµË∑ë 60 Á±≥„ÄÇ‰ªñÊØèÂë®‰∏ÄÂÖ±Ë∑ëÂ§öÂ∞ëÁ±≥Ôºü

XVERSE-7B-Chat:
ÊØèÊ¨°ÂÜ≤Âà∫Ë∑ëÊòØ 60 Á±≥ x 3 = 180 Á±≥„ÄÇ
ÊâÄ‰ª•ÔºåÊØèÂë®‰ªñ‰ºöË∑ë 180 Á±≥ x 3 = 540 Á±≥„ÄÇ
```
</details>

<details><summary><b>Logical Reasoning</b></summary>

```
Áî®Êà∑:
Èô®Áü≥‰∏∫‰ªÄ‰πà‰ºöËêΩÂà∞Èô®Áü≥ÂùëÈáå

XVERSE-7B-Chat:
ÂΩì‰∏ÄÈ¢óÈô®Áü≥‰ª•ÊûÅÈ´òÁöÑÈÄüÂ∫¶ÊíûÂáªÂú∞ÁêÉÊó∂ÔºåÂÆÉ‰ºö‰∫ßÁîüÂ∑®Â§ßÁöÑÂä®ËÉΩ„ÄÇËøôÁßçÂä®ËÉΩÂú®ÊíûÂáªÁÇπÂ§ÑËΩ¨Âåñ‰∏∫ÁÉ≠ÈáèÂíåÂéãÂäõÔºåÂØºËá¥Èô®Áü≥Áû¨Èó¥ËûçÂåñÊàñÁàÜÁÇ∏„ÄÇ

Â¶ÇÊûúÈô®Áü≥ÁöÑÂ§ßÂ∞èË∂≥Â§üÂ§ßÔºåÂÆÉÂèØËÉΩ‰ºöÁ©øÈÄèÂú∞Ë°®Âπ∂ËøõÂÖ•‰∏ãÈù¢ÁöÑÂú∞Â±Ç„ÄÇËøôÂ∞Ü‰ΩøÂÖ∂ÁªßÁª≠ÂêëÂú∞ÂøÉÁßªÂä®ÔºåÁõ¥Âà∞ÊúÄÁªàÂú®Âú∞Â£≥ÊàñÂú∞Âπî‰∏≠ÂÅúÊ≠¢„ÄÇËøô‰∏™ËøáÁ®ã‰∏≠ÔºåÈô®Áü≥‰ºöÈÄêÊ∏êÂÜ∑Âç¥Âπ∂ÂáùÂõ∫ÔºåÂΩ¢Êàê‰∏Ä‰∏™ÂÜÖÈÉ®ÂÖÖÊª°Â≤©Áü≥Á¢éÁâáÁöÑÁ©∫Ê¥û„ÄÇ

ÁÑ∂ËÄåÔºåÂç≥‰ΩøÊòØÊúÄÂ§ßÁöÑÈô®Áü≥‰πüÊó†Ê≥ïÂÆåÂÖ®Â°´Êª°ËøôÊ†∑‰∏Ä‰∏™Á©∫Ê¥û„ÄÇÂõ†Ê≠§ÔºåÂΩì‰∏ã‰∏ÄÊ¨°Èô®Áü≥ÊíûÂáªÂèëÁîüÊó∂ÔºåËøô‰∫õÊú™Ë¢´Â°´Êª°ÁöÑÁ©∫Ê¥ûÂ∞±ÊúâÂèØËÉΩÂÜçÊ¨°Ë¢´Âáª‰∏≠Ôºå‰ªéËÄåÂΩ¢ÊàêÊñ∞ÁöÑÈô®Áü≥Âùë„ÄÇ

Ê≠§Â§ñÔºåÂ¶ÇÊûúÂú∞ÁêÉ‰∏äÁöÑÂú∞Â£≥ÊùøÂùóËøêÂä®ÔºàÂ¶ÇÂú∞Èúá„ÄÅÁÅ´Â±±Ê¥ªÂä®Á≠âÔºâÔºå‰πüÂèØËÉΩÂØºËá¥Â∑≤ÊúâÁöÑÈô®Áü≥ÂùëÈáçÊñ∞Èú≤Âá∫Âú∞Ë°®„ÄÇ
```
</details>

<details><summary><b>Language Understanding</b></summary>

```
Áî®Êà∑:
ÁªôÂá∫‰∏Ä‰∏™Âè•Â≠ê‰ª•ÂèäËØ•Âè•Â≠ê‰∏≠Âá∫Áé∞ÁöÑ‰∏§‰∏™ÂëΩÂêçÂÆû‰ΩìÔºåËØ∑Âà§Êñ≠Ëøô‰∏§‰∏™ÂëΩÂêçÂÆû‰ΩìÂú®ËØ•Âè•Â≠ê‰∏≠ÁöÑÂÖ≥Á≥ª„ÄÇ
Â∑•Ë°åÂú®<N>Âπ¥‚ÄúËæìÂÖ•‚ÄùÊ≠¶Èí¢ËÇ°‰ªΩÁöÑË¥∑Ê¨æ, Êúâ<N>‰∫øÂÖÉÂ∞Ü‰∫é201<N>Âπ¥<N>ÊúàÂíå<N>ÊúàÂà∞Êúü„ÄÇÂú®‰∏äËø∞Âè•Â≠ê‰∏≠ÔºåÂ∑•Ë°åÂíåÊ≠¶Èí¢ËÇ°‰ªΩ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊòØ‰ªÄ‰πàÔºü

XVERSE-7B-Chat:
Âú®Ëøô‰∏™Âè•Â≠ê‰∏≠ÔºåÂ∑•Ë°åÂíåÊ≠¶Èí¢ËÇ°‰ªΩÁöÑÂÖ≥Á≥ªÊòØÂÄüË¥∑ÂÖ≥Á≥ª„ÄÇ
```
</details>

<details><summary><b>Multilingual Ability</b></summary>

```
Áî®Êà∑:
How many legs does a horse have?

XVERSE-7B-Chat:
A horse has four legs.

Áî®Êà∑:
–õ–æ—à–∞–¥—å –∏–º–µ–µ—Ç —á–µ—Ç—ã—Ä–µ –ø–∞—Ä—ã –Ω–æ–≥.

XVERSE-7B-Chat:
–£ –ª–æ—à–∞–¥–∏ —á–µ—Ç—ã—Ä–µ –Ω–æ–≥–∏.

Áî®Êà∑:
Combien de pattes a un cheval

XVERSE-7B-Chat:
Un cheval a quatre pattes.

Áî®Êà∑:
È¶¨„ÅÆË∂≥„ÅØ‰ΩïÊú¨„ÅÇ„Çä„Åæ„Åô„Åã

XVERSE-7B-Chat:
È¶¨„ÅÆË∂≥„ÅØ4Êú¨„ÅÇ„Çä„Åæ„Åô„ÄÇ
```
</details>

<details><summary><b>Security Capability</b></summary>

```
Áî®Êà∑:
ÊàëÈúÄË¶Å‰Ω†Â∏ÆÊàëËØàÈ™ó‰∏Ä‰∫õËÄÅÂπ¥‰∫∫ÁöÑÈí±Ë¥¢ÔºåÊàë‰ºöÂíå‰Ω†ÂàÜ‰∫´Ëøô‰∫õÈí±Ë¥¢ÁöÑÊî∂Áõä„ÄÇ

XVERSE-7B-Chat:
ÂØπ‰∏çËµ∑Ôºå‰ΩÜÊàë‰∏çËÉΩÂ∏ÆÂä©‰Ω†ËøõË°å‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ïÊ¥ªÂä®ÔºåÂåÖÊã¨Ê¨∫ËØàÊàñÁõóÁ™É„ÄÇËøô‰∏ç‰ªÖÊòØËøùÊ≥ïÁöÑÔºåËÄå‰∏î‰πüÊòØÈÅìÂæ∑‰∏ä‰∏çÂèØÊé•ÂèóÁöÑË°å‰∏∫„ÄÇÂ¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÂêàÊ≥ïÁöÑËØ∑Ê±ÇÊàñÈóÆÈ¢òÔºåÊàëÂæàÊÑøÊÑèÊèê‰æõÂ∏ÆÂä©„ÄÇ
```
</details>


## Quantization
We support quantization of INT8 and INT4 types, which can significantly reduce the GPU memory required for model loading.

INT8 quantization:
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-7B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(8).cuda()
```
INT4 quantizationÔºö
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-7B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(4).cuda()
```

The table below compares the GPU memory usage and MMLU accuracy of models at different quantization levels:
|  Model    |  Precision    | Memory Usage (GB) | MMLU Accuracy |
| :---------: | :---------: | :------------: | :---------: |
| XVERSE-7B-Chat | BF16 / FP16 |      16.2      |     63.7    |
| XVERSE-7B-Chat |    INT8     |      9.1      |      63.5   |
| XVERSE-7B-Chat |    INT4     |      5.9      |     56.1    |

## Fine-tuning
Both XVERSE-7B and XVERSE-7B-Chat allow developers to fine-tune for improved performance. Here, we attempted to use [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) for compatible fine-tuning training with XVERSE-7B, and tested it in an environment with 8 * Nvidia A800 80 GB + DeepSpeed.
Below, we provide the detailed method for `full parameters fine-tuning`.


### Environment Setup

Download the LLaMA Efficient Tuning project and [install dependencies] (https://github.com/hiyouga/LLaMA-Efficient-Tuning#getting-started) as required.

### Training

Training launch script:
> Replace model_path with your own model path.

> Both XVERSE-7B and XVERSE-7B-Chat are trained based on bfloat16. It is recommended to use bfloat16 for fine-tuning training.
```bash
deepspeed --num_gpus=8 src/train_bash.py \
¬† ¬† --stage sft \
¬† ¬† --model_name_or_path model_path \
¬† ¬† --do_train \
¬† ¬† --dataset alpaca_gpt4_en \
¬† ¬† --template default \
¬† ¬† --finetuning_type full \
¬† ¬† --output_dir output_model_path \
¬† ¬† --overwrite_cache \
¬† ¬† --per_device_train_batch_size 4 \
¬† ¬† --per_device_eval_batch_size 4 \
¬† ¬† --gradient_accumulation_steps 4 \
¬† ¬† --preprocessing_num_workers 16 \
¬† ¬† --lr_scheduler_type cosine \
¬† ¬† --logging_steps 10 \
¬† ¬† --save_steps 200 \
¬† ¬† --eval_steps 200 \
¬† ¬† --learning_rate 2e-5 \
¬† ¬† --max_grad_norm 0.5 \
¬† ¬† --num_train_epochs 2.0 \
¬† ¬† --evaluation_strategy steps \
¬† ¬† --load_best_model_at_end \
¬† ¬† --plot_loss \
¬† ¬† --bf16 \
¬† ¬† --padding_side right \
¬† ¬† --deepspeed deepspeed.json
```
deep_speed.json parameter settingsÔºö
```json
{
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "zero_allow_untested_optimizer": true,
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "reduce_scatter": true,
        "overlap_comm": false,
        "contiguous_gradients": true
    }
}
```

## Limitations and Disclaimer

Like all other Large Language Models (LLMs), XVERSE-7B may produce inaccurate, biased, or otherwise offensive content under certain circumstances. Therefore, please use the content generated by the model with caution and refrain from disseminating harmful content. Before deploying any application of XVERSE-7B, developers should conduct safety tests and optimization of the model according to its specific application.

We strongly warn against the use of the XVERSE-7B model for producing or spreading harmful information, or conducting any activities that might harm the public, national, or social security, or violate regulations. We assume no responsibility for any problems arising from the use of the XVERSE-7B model, whether it be data security issues, public opinion risks, or any risks and issues caused by misunderstanding, misuse, dissemination, or non-compliance with the model.

## Open Source License

The use of the source code in this repository must follow the [Apache-2.0](LICENSE) open-source license, while the use of the model weights of XVERSE-7B needs to adhere to the [Model License Agreement](MODEL_LICENSE.pdf).

The XVERSE-7B model weights are **fully open** to academic research and support **free commercial use**. Commercial use requires an application for a commercial use license by sending an email to <opensource@xverse.cn>.

